{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divar Scrap\n",
    "In this notebook, I try to scrap data from [Tehran Divar](https://divar.ir/s/tehran) website. \n",
    "\n",
    "This website is mainly designed for selling second hand stuffs, however after a while, there are some other services available in the website like housing.\n",
    "\n",
    "I focused on the [Tehran Divar](https://divar.ir/s/tehran), which is dedicated for *Tehran city*. I scrap housing advertisings in *Tehran city*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Unidecode in c:\\users\\a.alimadadi\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: arabic-reshaper in c:\\users\\a.alimadadi\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: future in c:\\users\\a.alimadadi\\anaconda3\\lib\\site-packages (from arabic-reshaper) (0.18.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\a.alimadadi\\anaconda3\\lib\\site-packages (from arabic-reshaper) (50.3.1.post20201107)\n",
      "Requirement already satisfied: progressbar in c:\\users\\a.alimadadi\\anaconda3\\lib\\site-packages (2.5)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install Unidecode\n",
    "from unidecode import unidecode\n",
    "\n",
    "!pip install arabic-reshaper\n",
    "from arabic_reshaper import reshape\n",
    "\n",
    "!pip install progressbar\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save_urls Method\n",
    "This method is used to find all advertisements of housing in *Tehran* and store their url in a text file called *AdsUrl.txt*. The url used to find all ads is [housing in Tehran Divar](https://divar.ir/s/tehran/rent-residential?warehouse=true). \n",
    "\n",
    "This page provides the user with around 24 or 25 housing advertisements in a 18.5\" monitor. To get more advertisements, user should scroll down the whole page to the bottom then new advertisements will be loaded in the same places.\n",
    "\n",
    "To achieve this goal, I use [Chromedriver](https://chromedriver.chromium.org/downloads). You need to change the chrome driver directory to your local path in the second line of code. By chrome driver, I scroll down the page each time and the advertisements are refreshed automatically.\n",
    "\n",
    "At the end, I store the url of each housing advertisement in a text file called *AdsUrl.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the urls of all advertisements\n",
    "# Web scrapper for infinite scrolling page #\n",
    "def save_urls(scroll_times = 100):\n",
    "    \n",
    "    with open(url_file, 'w', newline='', encoding='utf-8') as write_obj:\n",
    "                    write_obj.writelines('')\n",
    "            \n",
    "    # copy chrome driver in the main folder of project and paste its address in the line bellow\n",
    "    driver = webdriver.Chrome(executable_path=r\"D:\\Bank\\Educational\\Python\\Online Class\\13- Projects\\Divar\\Scrap\\chromedriver.exe\")\n",
    "    driver.get(search_url)\n",
    "    time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "    scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "    \n",
    "    # progress bar\n",
    "    bar = progressbar.ProgressBar(maxval=scroll_times, \\\n",
    "        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    \n",
    "    print('Finding links progress:')\n",
    "    bar.start()\n",
    "    \n",
    "    for i in range(scroll_times):\n",
    "        bar.update(i+1)\n",
    "\n",
    "        # scroll one screen height each time\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        time.sleep(scroll_pause_time)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # scrap the page\n",
    "        # find tag div for ads \n",
    "        for each_div in soup.find_all(\"div\",class_=\"post-card-item kt-col-6 kt-col-xxl-4\"):\n",
    "            if each_div == None : continue\n",
    "            url = ''\n",
    "\n",
    "            # find a tag\n",
    "            a_tag =  each_div.find(\"a\", recursive = False)\n",
    "\n",
    "            if a_tag != None and a_tag.has_attr('href'): \n",
    "                url = urljoin(home_url, a_tag.attrs['href'])\n",
    "                # find the rent urls and save in the text file\n",
    "                with open(url_file, 'a+', newline='', encoding='utf-8') as write_obj:\n",
    "                    write_obj.writelines(url + '\\n')\n",
    "    \n",
    "    bar.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrap_links Method\n",
    "In this method, I open the *AdsUrl.txt* file, prepared before, request each url and find all features of housing. \n",
    "\n",
    "Total 12 features will be found and stored in a csv fil called *Data.csv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap all links in url_file\n",
    "def scrap_links():\n",
    "    with open(url_file, 'r', newline='', encoding='utf-8') as read_obj:\n",
    "        links = read_obj.readlines()\n",
    "        \n",
    "        print('------------------------------')\n",
    "        print('Total link counts:',len(links))\n",
    "        # remove duplicates\n",
    "        links =  list(set(links))\n",
    "        print('Unique link counts:',len(links))\n",
    "        print('------------------------------')\n",
    "\n",
    "    # Write the headers in data csv file\n",
    "    with open(data_file, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        handle = csv.writer(csv_file)\n",
    "        handle.writerow(['neighborhood','area','year','room','deposit','rent','floor'\n",
    "            ,'elavator','parking','warehouse','balcony', 'link'])\n",
    "    \n",
    "    \n",
    "    # progress bar\n",
    "    bar1 = progressbar.ProgressBar(maxval= len(links), \\\n",
    "        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    \n",
    "    index_counter = 0\n",
    "    \n",
    "    print('Scraping links progress:')\n",
    "    bar1.start()\n",
    "\n",
    "    for each_link in links:\n",
    "        \n",
    "        bar1.update(index_counter+1)\n",
    "        index_counter += 1 \n",
    "        \n",
    "        each_link = each_link.replace('\\n','')\n",
    "        response = requests.get(each_link)\n",
    "        # Check if page is found\n",
    "        if response.status_code != 200: continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        neighborhood = area = year = room = deposit = rent = floor = ''\n",
    "        elavator = parking = warehouse = balcony = ''\n",
    "\n",
    "        # find main div containing features\n",
    "        post_div = soup.select('div.post-info')\n",
    "        if post_div == None or len(post_div) == 0: continue\n",
    "        temp_div = post_div[0].find('div', attrs={'class': None}, recursive = False)\n",
    "        if temp_div == None: continue\n",
    "        grouprow_divs = temp_div.find_all('div', attrs={'class': 'kt-group-row'}, recursive = False)\n",
    "        if grouprow_divs == None or len(grouprow_divs) < 2: continue\n",
    "        baserow_divs = temp_div.find_all('div', attrs={'class': 'kt-base-row kt-base-row--large kt-unexpandable-row'}, recursive = False)\n",
    "        if baserow_divs == None: continue\n",
    "        items_div = grouprow_divs[0].find_all('div', attrs={'class': 'kt-group-row-item kt-group-row-item--info-row'}, recursive = False)\n",
    "        if items_div == None: continue\n",
    "        \n",
    "        temp_div = post_div[0].find('div', attrs={'class': 'kt-page-title'}, recursive = False)\n",
    "        if temp_div == None: continue\n",
    "        temp_div = temp_div.find('div', attrs={'class': 'kt-page-title__texts'}, recursive = False)\n",
    "        if temp_div == None: continue\n",
    "        temp_div = temp_div.find('div', attrs={'class': 'kt-page-title__subtitle kt-page-title__subtitle--responsive-sized'}, recursive = False)\n",
    "        if temp_div == None: continue\n",
    "        neighborhood = temp_div.get_text()\n",
    "\n",
    "        index = 0\n",
    "        for each_div in items_div:\n",
    "            temp_span = each_div.find('span', attrs={'class': 'kt-group-row-item__value'}, recursive = False)\n",
    "            if temp_span == None: continue\n",
    "            if index == 0: area = temp_span.get_text()\n",
    "            if index == 1: year = temp_span.get_text()\n",
    "            if index == 2: room = temp_span.get_text()\n",
    "            index += 1\n",
    "        \n",
    "        index = 0\n",
    "        for each_div in baserow_divs:\n",
    "            temp_div = each_div.find('div', attrs={'class': 'kt-base-row__end kt-unexpandable-row__value-box'}, recursive = False)\n",
    "            if temp_div == None: continue\n",
    "            tmep_p = temp_div.find('p', attrs={'class': 'kt-unexpandable-row__value'}, recursive = False)\n",
    "            if tmep_p == None: continue\n",
    "            if index == 0: deposit = tmep_p.get_text()\n",
    "            if index == 1: rent = tmep_p.get_text()\n",
    "            if index == 5: floor = tmep_p.get_text()\n",
    "            index += 1\n",
    "        \n",
    "        index = 0\n",
    "        items_div = grouprow_divs[1].find_all('div', recursive = False)\n",
    "        for each_div in items_div:\n",
    "            temp_span = each_div.find('span', recursive = False)\n",
    "            temp_i = each_div.find('i', recursive = False)\n",
    "            if temp_i.has_attr(\"class\") == False: continue\n",
    "            if 'kt-icon-balcony' in temp_i['class']: balcony = temp_span.get_text()\n",
    "            if 'kt-icon-parking' in temp_i['class']: parking = temp_span.get_text()\n",
    "            if 'kt-icon-elevator' in temp_i['class']: elavator = temp_span.get_text()\n",
    "            if 'kt-icon-cabinet' in temp_i['class']: warehouse = temp_span.get_text()\n",
    "            index += 1\n",
    "        \n",
    "        # write in file\n",
    "        new_row = [neighborhood, area, year, room, deposit, rent, floor, elavator, parking\n",
    "                    , warehouse, balcony, each_link ]\n",
    "\n",
    "       \n",
    "        with open(data_file, 'a+', newline='', encoding='utf-8') as write_obj:\n",
    "            # Create a writer object from csv module\n",
    "            csv_writer = csv.writer(write_obj)\n",
    "            # Add contents of list as last row in the csv file\n",
    "            csv_writer.writerow(new_row)\n",
    "    \n",
    "    bar1.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_data Method\n",
    "In this method I do the folowings:\n",
    "1.   Romoving useless or bad records\n",
    "2.   Farsi characters correction\n",
    "3.   Extract useful data from phrases\n",
    "4.   Calculte the *total_value* column as the goal feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change farsi characters and clean data set\n",
    "def clean_data():\n",
    "    df = pd.read_csv('Data.csv', encoding=\"utf-8\")  \n",
    "    df.drop_duplicates(subset =None, keep = 'first', inplace = True)\n",
    "    \n",
    "    # filter apartments\n",
    "    # چون فقط آپارتمانها را می گیریم، بالکن ندارند\n",
    "    df = df[df['neighborhood'].str.contains('اجاره آپارتمان')]\n",
    "    df['neighborhood'] = df['neighborhood'].astype(pd.StringDtype())\n",
    "\n",
    "    # چون ستون طبقه، دارای مقادیر زیادی از نال و موارد نادرست است آن را حذف می کنیم\n",
    "    df.drop('floor', inplace=True, axis=1)\n",
    "    # ستون بالکون برای تمام موارد نال است\n",
    "    df.drop('balcony', inplace=True, axis=1)\n",
    "\n",
    "    # int columns\n",
    "    df['area'] = pd.to_numeric(df.area.apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "    df['room'] = pd.to_numeric(df.room.apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "\n",
    "    # string columns\n",
    "    df['warehouse'] = df['warehouse'].replace({'انباری ندارد': '۰'}, regex=True)\n",
    "    df['warehouse'] = df['warehouse'].replace({'انباری': '۱'}, regex=True)\n",
    "    df['warehouse'] = pd.to_numeric(df['warehouse'].apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "\n",
    "    df['elavator'] = df['elavator'].replace({'آسانسور ندارد': '۰'}, regex=True)\n",
    "    df['elavator'] = df['elavator'].replace({'آسانسور': '۱'}, regex=True)\n",
    "    df['elavator'] = pd.to_numeric(df['elavator'].apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "\n",
    "    df['parking'] = df['parking'].replace({'پارکینگ ندارد': '۰'}, regex=True)\n",
    "    df['parking'] = df['parking'].replace({'پارکینگ': '۱'}, regex=True)\n",
    "    df['parking'] = pd.to_numeric(df['parking'].apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "\n",
    "    df[['neighborhood','temp1']] = df['neighborhood'].str.split('،',expand=True)\n",
    "    df[['temp1','temp2']] = df['temp1'].str.split('|',expand=True)\n",
    "    df['neighborhood'] = df['temp1'].replace({'‌': ' '}, regex=True)\n",
    "\n",
    "    df['deposit'] = df['deposit'].replace({'مجانی': '۰'}, regex=True)\n",
    "    df['deposit'] = df['deposit'].replace({'توافقی': '۰'}, regex=True)\n",
    "    df['deposit'] = df['deposit'].replace({'٫': ''}, regex=True)\n",
    "    df['deposit'] = df['deposit'].replace({'تومان': ''}, regex=True)\n",
    "    df['deposit'] = pd.to_numeric(df['deposit'].apply(unidecode), errors='coerce').replace(np.nan, 0).astype(float)\n",
    "\n",
    "    df['rent'] = df['rent'].replace({'مجانی': '۰'}, regex=True)\n",
    "    df['rent'] = df['rent'].replace({'توافقی': '۰'}, regex=True)\n",
    "    df['rent'] = df['rent'].replace({'٫': ''}, regex=True)\n",
    "    df['rent'] = df['rent'].replace({'تومان': ''}, regex=True)\n",
    "    df['rent'] = pd.to_numeric(df['rent'].apply(unidecode), errors='coerce').replace(np.nan, 0).astype(float)\n",
    "\n",
    "    # قبل از 1370 را با 1363 پر می کنم تا فاصله ها حفظ شود\n",
    "    df['year'] = df['year'].replace({'قبل از ۱۳۷۰': '۱۳۶۳'}, regex=True)\n",
    "    df['year'] = pd.to_numeric(df.year.apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "\n",
    "    # تبدیل اجاره و ودیعه به یکدیگر و به دست آوردن یک عدد به عنوان ارزش منزل\n",
    "    df['total_value'] = ((df['rent'] * 100) / 3) + df['deposit']\n",
    "\n",
    "    # remove temp columns\n",
    "    df.drop(columns = ['temp1','temp2'], inplace=True, axis=1)\n",
    "\n",
    "    df = df[['total_value','neighborhood','area','year','deposit','rent','elavator','parking','warehouse']]\n",
    "    \n",
    "    df.to_csv(r'Data.csv', index = False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __main__ Method\n",
    "In the main method, I just called the *save_urls* and *scrap_links* respectively.\n",
    "\n",
    "Finally, the *Data.csv* file containing the features of housing ads will be prepared. This file can be used for analysis the housing price in all sections of *Tehran city*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    url_file = 'AdsUrl.txt'\n",
    "    data_file = 'Data.csv'\n",
    "    home_url = 'https://divar.ir'\n",
    "    search_url = \"https://divar.ir/s/tehran/rent-residential?warehouse=true\"\n",
    "\n",
    "    # 1- save the urls of advertisements in a file\n",
    "    save_urls(100)\n",
    "\n",
    "    # 2- read links from file and scrap all links\n",
    "    scrap_links()\n",
    "    \n",
    "    # 3- change farsi characters and clean data\n",
    "    clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "By running this program for many times, some tips are found:\n",
    "\n",
    "1.   500 times of scrolling seemed to be an optimal point.\n",
    "2.   By running the program in the early evening hours, I get more unique records.\n",
    "3.   Running the program will take about half an hour for 500 scroll times. \n",
    "4.   Running the program for 500 scroll times will return about 6000 unique records.\n",
    "5.   You can run this program in different days and combine the result in one csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
