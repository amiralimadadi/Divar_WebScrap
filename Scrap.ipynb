{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divar Scrap\n",
    "In this notebook, I try to scrap data from [Tehran Divar](https://divar.ir/s/tehran) website. \n",
    "\n",
    "This website is mainly designed for selling second hand stuffs, however after a while, there are some other services available in the website like housing.\n",
    "\n",
    "I focused on the [Tehran Divar](https://divar.ir/s/tehran), which is dedicated for *Tehran city*. I scrap housing advertisings in *Tehran city*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Unidecode in c:\\users\\m0jtabav1\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: arabic-reshaper in c:\\users\\m0jtabav1\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: progressbar in c:\\users\\m0jtabav1\\anaconda3\\lib\\site-packages (2.5)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\m0jtabav1\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\m0jtabav1\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\m0jtabav1\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\m0jtabav1\\appdata\\roaming\\python\\python311\\site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\m0jtabav1\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\m0jtabav1\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\m0jtabav1\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\m0jtabav1\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install Unidecode\n",
    "from unidecode import unidecode\n",
    "\n",
    "!pip install arabic-reshaper\n",
    "from arabic_reshaper import reshape\n",
    "\n",
    "!pip install progressbar\n",
    "import progressbar\n",
    "\n",
    "!pip install webdriver-manager\n",
    "# selenium 4.0\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save_urls Method\n",
    "This method is used to find all advertisements of housing in *Tehran* and store their url in a text file called *AdsUrl.txt*. The url used to find all ads is [housing in Tehran Divar](https://divar.ir/s/tehran/rent-residential?warehouse=true). \n",
    "\n",
    "This page provides the user with around 24 or 25 housing advertisements in a 18.5\" monitor. To get more advertisements, user should scroll down the whole page to the bottom then new advertisements will be loaded in the same places.\n",
    "\n",
    "To achieve this goal, I use [Chromedriver](https://chromedriver.chromium.org/downloads). You need to change the chrome driver directory to your local path in the second line of code. By chrome driver, I scroll down the page each time and the advertisements are refreshed automatically.\n",
    "\n",
    "At the end, I store the url of each housing advertisement in a text file called *AdsUrl.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the urls of all advertisements\n",
    "# Web scrapper for infinite scrolling page #\n",
    "def save_urls(scroll_times = 100):\n",
    "    \n",
    "    with open(url_file, 'w', newline='', encoding='utf-8') as write_obj:\n",
    "                    write_obj.writelines('')\n",
    "            \n",
    "    # copy chrome driver in the main folder of project and paste its address in the line bellow\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    driver.get(search_url)\n",
    "    time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "    scroll_pause_time = 5 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "    \n",
    "    # progress bar\n",
    "    bar = progressbar.ProgressBar(maxval=scroll_times, \\\n",
    "        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    \n",
    "    print('Finding links progress:')\n",
    "    bar.start()\n",
    "    \n",
    "    for i in range(scroll_times):\n",
    "        bar.update(i+1)\n",
    "\n",
    "        # scroll one screen height each time\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        try:\n",
    "                more_list = driver.find_element(By.XPATH, \"//button[@class='post-list__unsafe-show-more-e2b99']\")\n",
    "                more_list.click()\n",
    "                print(\"clicked on more list\")\n",
    "                time.sleep(1)\n",
    "        except:\n",
    "                pass\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # scrap the page\n",
    "        # find tag div for ads \n",
    "        for each_div in soup.find_all(\"div\",class_=\"post-list__widget-col-c1444\"):\n",
    "            if each_div == None : continue\n",
    "            url = ''\n",
    "\n",
    "            # find a tag\n",
    "            a_tag =  each_div.find(\"a\", recursive = False)\n",
    "\n",
    "            if a_tag != None and a_tag.has_attr('href'): \n",
    "                url = urljoin(home_url, a_tag.attrs['href'])\n",
    "                # find the rent urls and save in the text file\n",
    "                with open(url_file, 'a+', newline='', encoding='utf-8') as write_obj:\n",
    "                    write_obj.writelines(url + '\\n')\n",
    "    \n",
    "    bar.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrap_links Method\n",
    "In this method, I open the *AdsUrl.txt* file, prepared before, request each url and find all features of housing. \n",
    "\n",
    "Total 22 features will be found and stored in a csv fil called *Data.csv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap all links in url_file\n",
    "# try to scrap all the links in the file and retry if post_div is not found\n",
    "def scrap_links():\n",
    "    with open(url_file, 'r', newline='', encoding='utf-8') as read_obj:\n",
    "        links = read_obj.readlines()\n",
    "        \n",
    "        print('------------------------------')\n",
    "        print('Total link counts:',len(links))\n",
    "        # remove duplicates\n",
    "        links =  list(set(links))\n",
    "        print('Unique link counts:',len(links))\n",
    "        print('------------------------------')\n",
    "\n",
    "    # Write the headers in data csv file\n",
    "    with open(data_file, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        handle = csv.writer(csv_file)\n",
    "        handle.writerow(['id','name','neighborhood','area','year','room','deposit','rent','changeAble','buildingFloors'\n",
    "                ,'unitFloor','elavator','parking','warehouse','balcony','wc','cooling','heating','hotWater','unitPerFloor','direction','unitStatus','longitude','latitude','description','link'])\n",
    "    \n",
    "    \n",
    "    # progress bar\n",
    "    bar1 = progressbar.ProgressBar(maxval= len(links), \\\n",
    "        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    \n",
    "    index_counter = 0\n",
    "    \n",
    "    print('Scraping links progress:')\n",
    "    bar1.start()\n",
    "\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "    list_index = 0\n",
    "    for each_link in links:\n",
    "        list_index += 1\n",
    "        bar1.update(index_counter+1)\n",
    "        index_counter += 1 \n",
    "\n",
    "        neighborhood = area = year = room = deposit = rent = changeAble = buildingFloors = unitFloor = longitude = latitude = description = ''\n",
    "        elavator = parking = warehouse = balcony = wc = cooling = heating = hotWater = ''\n",
    "        unitPerFloor = direction = unitStatus = ''\n",
    "        \n",
    "        each_link = each_link.replace('\\n','')\n",
    "\n",
    "        driver.get(each_link)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        post_div = soup.select('div.kt-col-5')      \n",
    "        if len(post_div) == 0: continue\n",
    "        temp_section = post_div[0].find_all('section', recursive = False)\n",
    "        if len(temp_section) == 0: continue\n",
    "        temp_div = temp_section[0].find('div', attrs={'class': 'post-page__section--padded'}, recursive = False)\n",
    "        if temp_div == None: continue\n",
    "        baserow_divs = temp_div.find_all('div', attrs={'class': 'kt-base-row kt-base-row--large kt-unexpandable-row'}, recursive = False)\n",
    "        if len(baserow_divs) == 0: continue\n",
    "        grouprow_divs = temp_div.find_all('table', attrs={'class': 'kt-group-row'}, recursive = False)\n",
    "        if len(grouprow_divs) == 0: continue\n",
    "        # items_div = grouprow_divs.find_all('td', attrs={'class': 'kt-group-row-item kt-group-row-item__value kt-body kt-body--stable'}, recursive = False)\n",
    "\n",
    "        #get Neighborhood\n",
    "        temp_div = temp_section[0].find('div', attrs={'class': 'kt-page-title'}, recursive = False)\n",
    "        if temp_div == None: continue\n",
    "        temp_div = temp_div.find('div', attrs={'class': 'kt-page-title__texts'}, recursive = False)\n",
    "        if temp_div == None: continue\n",
    "        title_div = temp_div.find('div', attrs={'class': 'kt-page-title__title kt-page-title__title--responsive-sized'}, recursive = False)\n",
    "        if title_div == None: continue\n",
    "        name = title_div.get_text()\n",
    "        subtitle_div = temp_div.find('div', attrs={'class': 'kt-page-title__subtitle kt-page-title__subtitle--responsive-sized'}, recursive = False)\n",
    "        if subtitle_div == None: continue\n",
    "        neighborhood = subtitle_div.get_text()\n",
    "        # only select after '،' character\n",
    "        neighborhood = neighborhood.split('،')[1] \n",
    "\n",
    "        # get area, year, room\n",
    "        index = 0\n",
    "        items_div = grouprow_divs[0].find('tbody', recursive = False)\n",
    "        if items_div == None: continue\n",
    "        items_div = items_div.find('tr', attrs={'class': 'kt-group-row__data-row'}, recursive = False)\n",
    "        if items_div == None: continue\n",
    "        items_div = items_div.find_all('td', recursive = False)\n",
    "        for each_div in items_div:\n",
    "                if index == 0: area = each_div.get_text()\n",
    "                if index == 1: year = each_div.get_text()\n",
    "                if index == 2: room = each_div.get_text()\n",
    "                index += 1\n",
    "\n",
    "        # get price, ppm, floor\n",
    "        for each_div in baserow_divs:\n",
    "                temp_div_start = each_div.find('div', attrs={'class': 'kt-base-row__start kt-unexpandable-row__title-box'}, recursive = False)\n",
    "                temp_div_end = each_div.find('div', attrs={'class': 'kt-base-row__end kt-unexpandable-row__value-box'}, recursive = False)\n",
    "                if temp_div_start != None and temp_div_end != None: \n",
    "                        temp_p_start = temp_div_start.find('p', attrs={'class': 'kt-base-row__title'}, recursive = False)\n",
    "                        temp_p_end = temp_div_end.find('p', attrs={'class': 'kt-unexpandable-row__value'}, recursive = False)\n",
    "                if temp_p_start != None and temp_p_end != None:\n",
    "                        if temp_p_start.get_text() == 'ودیعه': deposit = temp_p_end.get_text()\n",
    "                        if temp_p_start.get_text() == 'اجارهٔ ماهانه': rent = temp_p_end.get_text()\n",
    "                        if temp_p_start.get_text() == 'ودیعه و اجاره': changeAble = temp_p_end.get_text()\n",
    "                        if temp_p_start.get_text() == 'طبقه': \n",
    "                                floor = temp_p_end.get_text()\n",
    "                                temp_floor = floor.split('از')\n",
    "                                if len(temp_floor) == 2: \n",
    "                                        buildingFloors = temp_floor[1].strip()\n",
    "                                        unitFloor = temp_floor[0].strip()\n",
    "\n",
    "\n",
    "        thead_div = grouprow_divs[1].find('thead', recursive = False)\n",
    "        if thead_div == None: continue\n",
    "        thead_tr = thead_div.find('tr', recursive = False)\n",
    "        if thead_tr == None: continue\n",
    "        thead_th = thead_tr.find_all('th', recursive = False)\n",
    "        if thead_th == None: continue\n",
    "        tbody_div = grouprow_divs[1].find('tbody', recursive = False)\n",
    "        if tbody_div == None: continue\n",
    "        tbody_tr = tbody_div.find('tr', recursive = False)\n",
    "        if tbody_tr == None: continue\n",
    "        tbody_td = tbody_tr.find_all('td', recursive = False)\n",
    "        index = 0\n",
    "        for each_th in thead_th:\n",
    "                td = tbody_td[index]\n",
    "                temp_i = each_th.find('i', recursive = False)\n",
    "                if temp_i == None: continue\n",
    "                if temp_i.has_attr(\"class\") == False: continue\n",
    "                if 'kt-icon-balcony' in temp_i['class']: balcony = td.get_text()\n",
    "                if 'kt-icon-parking' in temp_i['class']: parking = td.get_text()\n",
    "                if 'kt-icon-elevator' in temp_i['class']: elavator = td.get_text()\n",
    "                if 'kt-icon-cabinet' in temp_i['class']: warehouse = td.get_text()\n",
    "                index += 1\n",
    "\n",
    "        map_a = soup.select('a.map-cm__attribution')\n",
    "        if len(map_a) != 0: \n",
    "                # get href attribute\n",
    "                href = map_a[0]['href']\n",
    "                # url is like \"https://balad.ir/location?latitude=35.739093017052&amp;longitude=51.379365921021&radius=500\"\n",
    "                # get latitude and longitude\n",
    "                temp = href.split('latitude=')[1]\n",
    "                latitude = temp.split('&')[0]\n",
    "                temp = href.split('longitude=')[1]\n",
    "                longitude = temp.split('&')[0]\n",
    "                # remove &amp; from latitude and longitude\n",
    "                latitude = latitude.replace('&amp;','')\n",
    "                longitude = longitude.replace('&amp;','')\n",
    "                # remove radius from longitude\n",
    "                longitude = longitude.split('&radius')[0]\n",
    "\n",
    "\n",
    "        # click on detail_button\n",
    "        try:\n",
    "                more_details = driver.find_element(By.XPATH, \"//div[@class='raw-button-cd669']\")\n",
    "                if more_details != None:\n",
    "                        more_details.click()\n",
    "                        time.sleep(1)\n",
    "                        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                        temp_div_modal = soup.select('div.kt-modal__body')\n",
    "                        if len(temp_div_modal) != 0: \n",
    "                                # find all divs with class 'kt-base-row kt-base-row--large kt-unexpandable-row'\n",
    "                                temp_features_divs = temp_div_modal[0].find_all('div', attrs={'class': 'kt-base-row kt-base-row--large kt-unexpandable-row'}, recursive = False)\n",
    "                                if len(temp_features_divs) != 0:\n",
    "                                        for each_div in temp_features_divs:\n",
    "                                                temp_div_start = each_div.find('div', attrs={'class': 'kt-base-row__start kt-unexpandable-row__title-box'}, recursive = False)\n",
    "                                                temp_div_end = each_div.find('div', attrs={'class': 'kt-base-row__end kt-unexpandable-row__value-box'}, recursive = False)\n",
    "                                                if temp_div_start != None and temp_div_end != None: \n",
    "                                                        temp_p_start = temp_div_start.find('p', attrs={'class': 'kt-base-row__title'}, recursive = False)\n",
    "                                                        temp_p_end = temp_div_end.find('p', attrs={'class': 'kt-unexpandable-row__value'}, recursive = False)\n",
    "                                                if temp_p_start != None and temp_p_end != None:\n",
    "                                                        if temp_p_start.get_text() == 'تعداد واحد در طبقه': unitPerFloor = temp_p_end.get_text()\n",
    "                                                        if temp_p_start.get_text() == 'جهت ساختمان': direction = temp_p_end.get_text()\n",
    "                                                        if temp_p_start.get_text() == 'وضعیت واحد': unitStatus = temp_p_end.get_text()\n",
    "\n",
    "                                temp_ability_divs = temp_div_modal[0].find_all('div', attrs={'class': 'kt-base-row kt-base-row--large kt-base-row--has-icon kt-feature-row'}, recursive = False)\n",
    "                                if len(temp_ability_divs) != 0:\n",
    "                                        for each_div in temp_ability_divs:\n",
    "                                                temp_div = each_div.find('div', attrs={'class': 'kt-base-row__start'}, recursive = False)\n",
    "                                                if temp_div != None: \n",
    "                                                        temp_i = temp_div.find('i', recursive = False)\n",
    "                                                        temp_p = temp_div.find('p', recursive = False)\n",
    "                                                        if temp_i != None: \n",
    "                                                                if temp_i.has_attr(\"class\") != False:\n",
    "                                                                        if 'kt-icon-balcony' in temp_i['class']: balcony = temp_p.get_text()\n",
    "                                                                        if 'kt-icon-parking' in temp_i['class']: parking = temp_p.get_text()\n",
    "                                                                        if 'kt-icon-elevator' in temp_i['class']: elavator = temp_p.get_text()\n",
    "                                                                        if 'kt-icon-cabinet' in temp_i['class']: warehouse = temp_p.get_text()\n",
    "                                                                        if 'kt-icon-wc' in temp_i['class']: wc = temp_p.get_text()\n",
    "                                                                        if 'kt-icon-snowflake' in temp_i['class']: cooling = temp_p.get_text()\n",
    "                                                                        if 'kt-icon-sunny' in temp_i['class']: heating = temp_p.get_text()\n",
    "                                                                        if 'kt-icon-thermometer' in temp_i['class']: hotWater = temp_p.get_text()\n",
    "        except:\n",
    "                pass\n",
    "\n",
    "        \n",
    "        # write in file\n",
    "        new_row = [index_counter, name, neighborhood, area, year, room, deposit, rent, changeAble, buildingFloors\n",
    "                , unitFloor, elavator, parking, warehouse, balcony, wc, cooling, heating, hotWater, unitPerFloor, direction, unitStatus, longitude, latitude, description, each_link ]\n",
    "        \n",
    "        with open(data_file, 'a+', newline='', encoding='utf-8') as write_obj:\n",
    "                # Create a writer object from csv module\n",
    "                csv_writer = csv.writer(write_obj)\n",
    "                # Add contents of list as last row in the csv file\n",
    "                csv_writer.writerow(new_row)\n",
    "\n",
    "        bar1.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_data Method\n",
    "In this method I do the folowings:\n",
    "1.   Romoving useless or bad records\n",
    "2.   Farsi characters correction\n",
    "3.   Extract useful data from phrases\n",
    "4.   Calculte the *total_value* column as the goal feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change farsi characters and clean data set\n",
    "def clean_data():\n",
    "\n",
    "    # read all data with clean in their name\n",
    "    path = './Data'\n",
    "    files = os.listdir(path)\n",
    "    files = [f for f in files if f.startswith('Data_'+ city) and f.endswith('.csv')]\n",
    "    df = pd.concat([pd.read_csv(os.path.join(path, f)) for f in files], ignore_index=True)\n",
    "    print(\"After concat: \", df.shape)\n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(subset =None, keep = 'first', inplace = True)\n",
    "\n",
    "    print(\"After drop_duplicates: \", df.shape)\n",
    "\n",
    "    df['neighborhood'] = df['neighborhood'].str.strip()\n",
    "\n",
    "    # drop rows with deposit and rent empty values\n",
    "    df = df.dropna(subset=['deposit', 'rent', 'area', 'room', 'year'])\n",
    "    print(\"After dropna [deposit, rent, area, room, year]: \", df.shape)\n",
    "\n",
    "    # drop rows with deposit and rent = 0\n",
    "    df = df[(df['deposit'] != 0) & (df['rent'] != 0)]\n",
    "    print(\"After drop [deposit, rent] = 0: \", df.shape)\n",
    "\n",
    "    # drop rows with Nan neighborhood and longitude and latitude at same time\n",
    "    df = df.dropna(subset=['neighborhood'])\n",
    "    print(\"After dropna [neighborhood]: \", df.shape)\n",
    "\n",
    "    # set id column as index and reset index\n",
    "    df = df.set_index('id')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # drop wc, cooling, heating, hotWater columns\n",
    "    df = df.drop(['changeAble', 'wc', 'cooling', 'heating', 'hotWater'], axis=1)\n",
    "\n",
    "    # int columns\n",
    "    # replace empty area and room with 0\n",
    "    df['area'] = df['area'].replace(np.nan, 0).astype(int)\n",
    "    \n",
    "    # df['area'] = pd.to_numeric(df['area'].apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "    df['room'] = df['room'].replace({'بدون اتاق': '0'}, regex=True)\n",
    "    df['room'] = df['room'].replace({'بیشتراز۸': '8'}, regex=True)\n",
    "    df['room'] = df['room'].replace(np.nan, 0).astype(int)\n",
    "    # df['room'] = pd.to_numeric(df['room'].apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "\n",
    "    # string columns\n",
    "    df['buildingFloors'] = df['buildingFloors'].replace({'': '1'}, regex=True)\n",
    "    df['buildingFloors'] = df['buildingFloors'].replace({'بیشتر از ۱۵': '15'}, regex=True)\n",
    "    df['buildingFloors'] = df['buildingFloors'].replace({'بیشتر از ۱۰': '10'}, regex=True)\n",
    "    df['buildingFloors'] = df['buildingFloors'].replace({'بیشتر از ۵': '5'}, regex=True)\n",
    "    df['buildingFloors'] = df['buildingFloors'].replace({'بیشتر از ۳': '3'}, regex=True)\n",
    "    df['buildingFloors'] = df['buildingFloors'].replace({'انتخاب نشده': '1'}, regex=True)\n",
    "    df['buildingFloors'] = df['buildingFloors'].replace(np.nan, 0).astype(int)\n",
    "\n",
    "    df['warehouse'] = df['warehouse'].replace({'انباری ندارد': '0'}, regex=True)\n",
    "    df['warehouse'] = df['warehouse'].replace({'انباری': '1'}, regex=True)\n",
    "    df['warehouse'] = df['warehouse'].replace({'': '0'}, regex=True)\n",
    "    df['warehouse'] = df['warehouse'].replace(np.nan, 0).astype(int)\n",
    "    # df['warehouse'] = pd.to_numeric(df['warehouse'].apply(unidecode), errors='coerce')\n",
    "\n",
    "    df['elavator'] = df['elavator'].replace({'آسانسور ندارد': '0'}, regex=True)\n",
    "    df['elavator'] = df['elavator'].replace({'آسانسور': '1'}, regex=True)\n",
    "    df['elavator'] = df['elavator'].replace({'': '0'}, regex=True)  \n",
    "    df['elavator'] = df['elavator'].replace(np.nan, 0).astype(int)\n",
    "    # df['elavator'] = pd.to_numeric(df['elavator'].apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "\n",
    "    df['parking'] = df['parking'].replace({'پارکینگ ندارد': '0'}, regex=True)\n",
    "    df['parking'] = df['parking'].replace({'پارکینگ': '1'}, regex=True)\n",
    "    df['parking'] = df['parking'].replace({'': '0'}, regex=True)\n",
    "    df['parking'] = df['parking'].replace(np.nan, 0).astype(int)     \n",
    "    # df['parking'] = pd.to_numeric(df['parking'].apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "\n",
    "    df['warehouse'] = df['warehouse'].replace({'انباری ندارد': '0'}, regex=True)\n",
    "    df['warehouse'] = df['warehouse'].replace({'انباری دارد': '1'}, regex=True)\n",
    "    df['warehouse'] = df['warehouse'].replace({'انباری': '1'}, regex=True)\n",
    "    df['warehouse'] = df['warehouse'].replace({'': '0'}, regex=True)\n",
    "    df['warehouse'] = df['warehouse'].replace(np.nan, 0).astype(int) \n",
    "\n",
    "    df['balcony'] = df['balcony'].replace({'بالکن ندارد': '0'}, regex=True)\n",
    "    df['balcony'] = df['balcony'].replace({'بالکن دارد': '1'}, regex=True)\n",
    "    df['balcony'] = df['balcony'].replace({'بالکن': '1'}, regex=True)\n",
    "    df['balcony'] = df['balcony'].replace({'1 انتخاب نشده': '0'}, regex=True)\n",
    "    df['balcony'] = df['balcony'].replace({'': '0'}, regex=True)\n",
    "    df['balcony'] = df['balcony'].replace(np.nan, 0).astype(int) \n",
    "\n",
    "    df['deposit'] = df['deposit'].replace({'مجانی': '0'}, regex=True)\n",
    "    df['deposit'] = df['deposit'].replace({'توافقی': '0'}, regex=True)\n",
    "    df['deposit'] = df['deposit'].replace({'٬': ''}, regex=True)\n",
    "    df['deposit'] = df['deposit'].replace({'تومان': ''}, regex=True)\n",
    "    df['deposit'] = df['deposit'].replace({' ': ''}, regex=True)\n",
    "    df['deposit'] = df['deposit'].replace(np.nan, 0).astype(float) \n",
    "    # df['deposit'] = pd.to_numeric(df.deposit.apply(unidecode), errors='coerce').replace(np.nan, 0).astype(float)\n",
    "\n",
    "    df['rent'] = df['rent'].replace({'مجانی': '0'}, regex=True)\n",
    "    df['rent'] = df['rent'].replace({'توافقی': '0'}, regex=True)\n",
    "    df['rent'] = df['rent'].replace({'٬': ''}, regex=True)\n",
    "    df['rent'] = df['rent'].replace({'تومان': ''}, regex=True)\n",
    "    df['rent'] = df['rent'].replace({' ': ''}, regex=True)\n",
    "    df['rent'] = df['rent'].replace(np.nan, 0).astype(float) \n",
    "    # df['rent'] = pd.to_numeric(df.rent.apply(unidecode), errors='coerce').replace(np.nan, 0).astype(float)\n",
    "\n",
    "    # قبل از 1370 را با 1363 پر می کنم تا فاصله ها حفظ شود\n",
    "    df['year'] = df['year'].replace({'قبل از ۱۳۷۰': '۱۳۶۳'}, regex=True)\n",
    "    df['year'] = df['year'].replace(np.nan, 0).astype(int)\n",
    "    # df['year'] = pd.to_numeric(df.year.apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "\n",
    "    # convert unitFloor to int\n",
    "    df['unitFloor'] = df['unitFloor'].replace({' ': ''}, regex=True)\n",
    "    df['unitFloor'] = df['unitFloor'].replace({'': '0'}, regex=True)\n",
    "    df['unitFloor'] = df['unitFloor'].replace({'همکف': '0'}, regex=True)\n",
    "    df['unitFloor'] = df['unitFloor'].replace({'زیر0': '-1'}, regex=True)\n",
    "    df['unitFloor'] = df['unitFloor'].replace(np.nan, 0).astype(int)\n",
    "    # df['unitFloor'] = pd.to_numeric(df.unitFloor.apply(unidecode), errors='coerce').replace(np.nan, 0).astype(int)\n",
    "\n",
    "    df['unitPerFloor'] = df['unitPerFloor'].replace({' ': ''}, regex=True)\n",
    "    df['unitPerFloor'] = df['unitPerFloor'].replace({'': '1'}, regex=True)\n",
    "    df['unitPerFloor'] = df['unitPerFloor'].replace({'انتخابنشده': '1'}, regex=True)\n",
    "    df['unitPerFloor'] = df['unitPerFloor'].replace({'بیشتراز۸': '1'}, regex=True)\n",
    "    df['unitPerFloor'] = df['unitPerFloor'].replace(np.nan, 0).astype(int)\n",
    "\n",
    "    df['unitStatus'] = df['unitStatus'].replace({'بازسازی نشده': '0'}, regex=True)\n",
    "    df['unitStatus'] = df['unitStatus'].replace({'بازسازی شده': '1'}, regex=True)\n",
    "    df['unitStatus'] = df['unitStatus'].replace({' ': ''}, regex=True)\n",
    "    df['unitStatus'] = df['unitStatus'].replace(np.nan, 0).astype(int)\n",
    "    \n",
    "    df['direction'] = df['direction'].replace({' ': ''}, regex=True)\n",
    "    df['direction'] = df['direction'].replace({'شمالی': '1'}, regex=True)\n",
    "    df['direction'] = df['direction'].replace({'جنوبی': '0'}, regex=True)\n",
    "\n",
    "    # convert longitude and latitude to float\n",
    "    df['longitude'] = df['longitude'].replace(np.nan, 0).astype(float)\n",
    "    df['latitude'] = df['latitude'].replace(np.nan, 0).astype(float)\n",
    "\n",
    "    # تبدیل اجاره و ودیعه به یکدیگر و به دست آوردن یک عدد به عنوان ارزش منزل\n",
    "    df['total_value'] = ((df['rent'] * 3) / 100) + df['deposit']\n",
    "\n",
    "    # drop raw with total_value = 0\n",
    "    df = df[df['total_value'] != 0]\n",
    "\n",
    "\n",
    "    # df = df[['total_value','neighborhood','area','year','deposit','rent','elavator','parking','room','unitFloor','longitude','latitude']]\n",
    "    \n",
    "    # save the combined data\n",
    "    df.to_csv(data_combined_file, index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __main__ Method\n",
    "In the main method, I just called the *save_urls* and *scrap_links* respectively.\n",
    "\n",
    "Finally, the *Data.csv* file containing the features of housing ads will be prepared. This file can be used for analysis the housing price in all sections of *Tehran city*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After concat:  (43164, 27)\n",
      "After drop_duplicates:  (39744, 27)\n",
      "After dropna [deposit, rent, area, room, year]:  (28491, 27)\n",
      "After drop [deposit, rent] = 0:  (26121, 27)\n",
      "After dropna [neighborhood]:  (26121, 27)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Specific the City\n",
    "    city = 'tehran'\n",
    "    # Create Directory for Urls and Data if not exist\n",
    "    if not os.path.exists('Urls'):\n",
    "        os.makedirs('Urls')\n",
    "    if not os.path.exists('Data'):\n",
    "        os.makedirs('Data')\n",
    "    # get timestamp \n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # timestamp = \"20240227-183741\"\n",
    "    # Add timestamp to Urls file names\n",
    "    url_file = './Urls/' + 'AdsUrls_' + city + '_'  +  timestamp + '.txt'\n",
    "    data_file = './Data/' + 'Data_' +  city + '_' + timestamp  + '.csv'\n",
    "    data_clean_file = './Data/' + 'Data_' +  city + '_' + 'clean_' + timestamp  + '.csv'\n",
    "    data_combined_file = './Data/' + 'Dataset_' +  city + '_' + timestamp +'.csv'\n",
    "    # Search Urls\n",
    "    home_url = 'https://divar.ir'\n",
    "    search_url = \"https://divar.ir/s/\" + city  + \"/rent-apartment\"\n",
    "\n",
    "    #1- save the urls of advertisements in a file\n",
    "    save_urls(500)\n",
    "\n",
    "    # 2- read links from file and scrap all links\n",
    "    scrap_links()\n",
    "    \n",
    "    # 3- change farsi characters and clean data\n",
    "    clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "By running this program for many times, some tips are found:\n",
    "\n",
    "1.   500 times of scrolling seemed to be an optimal point.\n",
    "2.   By running the program in the early evening hours, I get more unique records.\n",
    "3.   Running the program will take about half an hour for 500 scroll times. \n",
    "4.   Running the program for 500 scroll times will return about 6000 unique records.\n",
    "5.   You can run this program in different days and combine the result in one csv file."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
